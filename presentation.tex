\documentclass{beamer}
\usetheme[pageofpages=of,% String used between the current page and the
                         % total page count.
          bullet=circle,% Use circles instead of squares for bullets.
          titleline=true,% Show a line below the frame title.
          alternativetitlepage=true,% Use the fancy title page.
       %   titlepagelogo=logo-polito,% Logo for the first page.
       %   watermark=watermark-polito,% Watermark used in every page.
       %   watermarkheight=100px,% Height of the watermark.
       %   watermarkheightmult=4,% The watermark image is 4 times bigger
                                % than watermarkheight.
          ]{Torino}

\author{Brendon J. Brewer}
\title{Bayesian probability theory}
\institute{Department of Statistics, The University of Auckland}
\date{{\color{blue} https://www.stat.auckland.ac.nz/\~{ }brewer}\\
\vspace{10pt}
{\color{blue} @brendonbrewer}}

\usepackage{dsfont}

\begin{document}

\begin{frame}[t,plain]
\titlepage
\end{frame}

\begin{frame}[t]{Propositions}
Propositions are {\it statements that can be either true or false}.\\
Examples:\vspace{20pt}

$A$: {\small Hillary Clinton will be president of the USA on June 14th, 2017.}\\
$B$: {\small The current temperature in Auckland is greater than 14$^{\circ}$C.}\\
$C$: {\small $\Omega_\Lambda \neq 0$.}\\
$D$: {\small David Hogg has had more than 500 mg of caffeine today.}
\end{frame}

\begin{frame}[t]{Propositions}
Propositions can be {\it combined} to make others, using the operators
{\bf and} ($\wedge$), {\bf or} ($\vee$), and {\bf not} ($\neg$): \vspace{20pt}

$A \wedge B$\\
$C \vee A$\\
$\neg D$\\
$\neg (A \vee (C \wedge D))$
\end{frame}

\begin{frame}[t]{Notation}
The operator {\bf and} is sometimes denoted with a comma.\vspace{20pt}

\begin{equation}
(A \wedge B) \equiv (A, B)
\end{equation}

\end{frame}

\begin{frame}[t]{Quantifying propositions}
Science is largely concerned with the {\it plausibility} of propositions.
Plausibility depends on the information you have, so is a function of {\it
two} propositions. Examples:\vspace{20pt}

$P(A | B)$\\
$P(\neg D | (C \vee D))$\\
$P((C \wedge D) | \neg A)$\vspace{20pt}

Read the ``$|$'' as ``{\bf given}'' or ``{\bf conditional on}''.
\end{frame}


\begin{frame}[t]{Constraints on plausibility}
Some properties of plausibility:\vspace{20pt}

$P(A \vee B) \geq P(A)$\\
%$P(A \wedge B) \leq P(A)$\\
$P(A \vee (B \vee C)) = P((A \vee B) \vee C)$\vspace{20pt}

Where'd the RHS go? $|| D$
\end{frame}

\begin{frame}[t]{Constraints on plausibility}
Some properties of plausibility:\vspace{20pt}

$P(A \vee B) \geq P(A)$\\
$P(A \wedge B) \leq P(A)$\\
$P(A \vee (B \vee C)) = P((A \vee B) \vee C)$\vspace{20pt}

These imply the {\bf sum rule} and the {\bf product rule}, and hence that
{\bf plausibilities are probabilities}.
\end{frame}

\begin{frame}[t]{Sum and product rule\hspace{170pt}$|| C$}
$P(A \vee B) = P(A) + P(B) - P(A \wedge B)$\\
\vspace{20pt}
$P(A \wedge B) = P(A)P(B|A)$
\end{frame}

\begin{frame}[t]{Bayes' Rule\hspace{170pt}$|| I$}
Bayes' Rule (consequence of product rule and commutativity of {\bf and})
\begin{equation}
P(H | D) = \frac{P(H)P(D|H)}{P(D)}
\end{equation}
where\vspace{20pt}
$P(D) = P(H)P(D | H) + P(\neg H)P(D | \neg H)$
(consequence of sum rule)
\end{frame}

\begin{frame}[t]{Terminology\hspace{170pt}$|| I$}
\begin{equation}
P(H | D) = \frac{P(H)P(D|H)}{P(D)}
\end{equation}
\begin{equation}
\textnormal{(posterior probability)} =
\frac{\textnormal{(prior probability)}\times\textnormal{(likelihood)}}{\textnormal{(marginal likelihood)}}
\end{equation}
\end{frame}

\begin{frame}[t]{Another Bayes' Rule\hspace{170pt}$|| I$}
For $N$ mutually exclusive, exhaustive hypotheses
$H_1, H_2, ..., H_N$, we have $N$ posterior probabilities:

\begin{equation}
P(H_i | D) = \frac{P(H_i)P(D|H_i)}{P(D)}
\end{equation}

where
\begin{equation}
P(D) = \sum_{i=1}^N P(H_i)P(D|H_i)
\end{equation}
\end{frame}

\begin{frame}[t]{Another Bayes' Rule\hspace{170pt}$|| I$}

{\bf This is the most important form of Bayes' rule.}

\begin{equation}
P(H_i | D) = \frac{P(H_i)P(D|H_i)}{P(D)}
\end{equation}

where
\begin{equation}
P(D) = \sum_{i=1}^N P(H_i)P(D|H_i)
\end{equation}
\end{frame}


\begin{frame}[t]{Exercises!\hspace{170pt}$|| I$}
\end{frame}

\begin{frame}[t]{Parameter estimation}
In most applications, we can use the ``parameter estimation'' story. E.g.
Let $\theta$ be a quantity we want to know. Then the hypotheses might be:

\begin{eqnarray}
H_1 &\equiv& \theta = 5\\
H_2 &\equiv& \theta = 6\\
H_3 &\equiv& \theta = 7
\end{eqnarray}

The data could also be a number. E.g. the conditioning proposition could be
\begin{equation}
D = 4
\end{equation}

The previous version of Bayes' rule can be applied to each of the.
\end{frame}

\begin{frame}[t]{Parameter estimation}
Define the prior distribution $p(\theta)$


\begin{equation}
p(\theta | D) \propto p(\theta)p(D|\theta)
\end{equation}

This notation hides {\bf a lot}!
\end{frame}


\begin{frame}[t]{Ingredients I}
Bayesian inference need the following inputs:

\begin{itemize}
\setlength{\itemsep}{20pt}
\item A {\bf hypothesis space} describing the set of possible answers to our
question (``parameter space'' in fitting is the same concept).
\item A {\bf prior distribution} $p(\theta)$ describing how plausible
each of the possible solutions is, not taking into account the data.
\end{itemize}
\end{frame}

\begin{frame}[t]{Ingredients II}
Bayesian inference need the following inputs:
\begin{itemize}
\item A {\bf sampling distribution} $p(D | \theta)$ describing our knowledge
about the connection between the parameters and the data.
\end{itemize}

When $D$ is known,
this is a function of $\theta$ called the {\bf likelihood}.
\end{frame}


\begin{frame}[t]{The Posterior Distribution}
The data helps us by changing our prior distribution to the {\bf posterior
distribution}, given by
\begin{eqnarray*}
p(\theta | D) &=& \frac{p(\theta) p(D|\theta)}{p(D)}
\end{eqnarray*}
where the denominator is the normalisation constant, usually called either
the {\bf marginal likelihood} or the {\bf evidence}.
\begin{eqnarray*}
p(D) &=& \int p(\theta)p(D|\theta) \, d\theta.
\end{eqnarray*}

\end{frame}

\begin{frame}[t]{Posterior Distribution vs. Maximum Likelihood}
The practical difference between these two concepts is greater in higher
dimensional problems.
\begin{center}
\includegraphics[scale=0.35]{bayes.pdf}
\end{center}
\end{frame}




\begin{frame}[t]{Updating Probabilities: Example}
\begin{center}
\includegraphics[scale=0.5]{ebola.jpg}
\end{center}
A patient goes to the doctor because he as a fever. Define
\begin{center}
\begin{tabular}{ll}
$H \equiv $ & ``The patient has Ebola''\\
$\neg H \equiv $ & ``The patient does not have Ebola''.
\end{tabular}
\end{center}

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Based on all of her knowledge, the doctor assigns probabilities to the two
hypotheses.
\begin{eqnarray*}
P(H) &=& 0.01\\
P(\neg H) &=& 0.99
\end{eqnarray*}

But she wants to test the patient to make sure.
\end{frame}



\begin{frame}[t]{Updating Probabilities: Example}
The patient is tested. Define

\begin{center}
\begin{tabular}{ll}
$D \equiv $ & ``The {\bf test says} the patient has Ebola''\\
$\neg D \equiv $ & ``The {\bf test says} the patient does not have Ebola''.
\end{tabular}
\end{center}

If the test were perfect, we'd have $P(D | H) = 1$, $P(\neg D | H) = 0$,
$P(D | \neg H) = 0$, and $P(\neg D | \neg H) = 1$.
\end{frame}


\begin{frame}[t]{Updating Probabilities: Example}
The Ebola test isn't perfect. Suppose there's a 5\% probability it simply gives
the wrong answer. Then we have:

\begin{eqnarray*}
P(D | H)   &=& 0.95\\
P(\neg D | H) &=& 0.05\\
P(D | \neg H)   &=& 0.05\\
P(\neg D | \neg H) &=& 0.95
\end{eqnarray*}

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Overall, there are four possibilities, considering whether the patient has
Ebola or not, and what the test says.

\begin{center}
$(H, D)$\\
$(\neg H, D)$\\
$(H, \neg D)$\\
$(\neg H, \neg D)$
\end{center}


\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
The probabilities for these four possibilities can be found using the product
rule.
\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\neg H, D) &=& 0.99 \times 0.05\\
P(H, \neg D) &=& 0.01 \times 0.05\\
P(\neg H, \neg D) &=& 0.99 \times 0.95\\
\end{eqnarray*}
\vspace{-45pt}

These four possibilities are {\bf mutually exclusive} (only one of them is true)
and exhaustive (it's not ``something else''), so the probabilities add up to 1.

\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
The test results come back and say that the patient has Ebola. That is, we've
learned that $D$ is true. So we can confidently rule out those possibilities
where $D$ is false:

\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\neg H, D) &=& 0.99 \times 0.05\\
{\color{red} P(H, \neg D)} &=& {\color{red} 0.01 \times 0.05}\\
{\color{red} P(\neg H, \neg D)} &=& {\color{red} 0.99 \times 0.95}\\
\end{eqnarray*}


\end{frame}


\begin{frame}[t]{Updating Probabilities: Example}
We are left with these two possibilities.

\begin{eqnarray*}
P(H, D) &=& 0.01 \times 0.95\\
P(\neg H, D) &=& 0.99 \times 0.05
\end{eqnarray*}

It would be strange to modify these probabilities just because we deleted the
other two. The only thing we have to do is renormalise them, by dividing by the total, so they sum to 1 again.
\end{frame}

\begin{frame}[t]{Updating Probabilities: Example}
Normalising, we get

\begin{eqnarray*}
P(H | D) &=& (0.01 \times 0.95)/(0.01 \times 0.95 + 0.99\times0.05) = 0.161\\
P(\neg H | D) &=& (0.99 \times 0.05)/(0.01 \times 0.95 + 0.99\times0.05) = 0.839
\end{eqnarray*}
\end{frame}

\begin{frame}[t]{Moral}
Bayesian updating is completely equivalent to:
\begin{itemize}
\item Writing a list of possible answers to your question
\item Giving a probability to each
\item Deleting the ones that you discover are false.
\end{itemize}

It just seems more complicated than this because we often apply it to more
complex sets of hypotheses.
\end{frame}


\begin{frame}[t]{Part II: Nested Sampling}
Nested Sampling is a Monte Carlo method (not necessarily MCMC) that was
introduced by John Skilling in 2004.\\
\vspace{20pt}
It is very popular in astrophysics and has some unique strengths.
\end{frame}


\begin{frame}[t]{Marginal Likelihood}
The {\bf marginal likelihood} is useful for ``model selection''. Consider
two models: $M_1$ with parameters $\theta_1$, $M_2$ with parameters $\theta_2$.
The marginal likelihoods are:
\begin{eqnarray*}
p(D | M_1) &=& \int p(\theta_1 | M_1) p(D | \theta_1, M_1) \, d\theta_1\\
p(D | M_2) &=& \int p(\theta_2 | M_2) p(D | \theta_2, M_2) \, d\theta_2
\end{eqnarray*}

These are the normalising constants of the posteriors, within each model.
\end{frame}


\begin{frame}[t]{Bayesian Model Selection}
If you have the marginal likelihoods, it's easy:

\begin{eqnarray*}
\frac{P(M_1 | D)}{P(M_2 | D)} &=& \frac{P(M_1)}{P(M_2)}
\times \frac{P(D | M_1)}{P(D | M_2)}.
\end{eqnarray*}

\begin{eqnarray*}
\textnormal{(posterior odds)} = \textnormal{(prior odds)} \times \textnormal{(bayes factor)}
\end{eqnarray*}

\end{frame}


\begin{frame}[t]{Challenging features}
Another motivation: standard MCMC methods can get stuck in the following
situations:
\begin{center}
\includegraphics[scale=0.4]{challenges.pdf}
\end{center}
\end{frame}

\begin{frame}{Nested Sampling}
Nested Sampling was built to estimate the marginal likelihood.

But it can also be used to generate posterior samples, and it can potentially
work on harder problems where standard MCMC methods get stuck.
\end{frame}

\begin{frame}[t]{Notation}
When discussing Nested Sampling, we use different symbols:
\begin{eqnarray*}
p(D | M_1) &=& \int p(\theta_1 | M_1) p(D | \theta_1, M_1) \, d\theta_1\\
\end{eqnarray*}
becomes
\begin{eqnarray*}
Z &=& \int \pi(\theta) L(\theta) \, d\theta.
\end{eqnarray*}

$Z$ = marginal likelihood, $L(\theta)$ = likelihood function, $\pi(\theta)$ = prior
distribution.
\end{frame}


\begin{frame}[t]{Nested Sampling}
Imagine we had an easy 1-D problem, with a Uniform(0, 1) prior, and a likelihood
that was strictly decreasing.

%\begin{center}
%\begin{figure}
%	\includegraphics[scale=0.28,clip=true,angle=0]{Patricio/AreaZ.pdf}
%\caption{Likelihood function with area Z.}
%\end{figure}
%\end{center}

\end{frame}


\begin{frame}[t]{Nested Sampling}
The key idea of Nested Sampling: Our high dimensional problem can be mapped
onto the easy 1-D problem. Figure from Skilling (2006):

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{ns.png}
\end{center}
\end{figure}

\end{frame}

\begin{frame}{Nested Sampling $X$}
\vspace{-20pt}
Define
\begin{eqnarray*}
X(L^*) = \int \pi(\theta) \mathds{1}\left(L(\theta) > L^*\right)\, d\theta
\end{eqnarray*}

\vspace{10pt}

$X$ is the {\bf amount of prior probability} with likelihood greater than $L^*$.
Loosely, $X$ is the {\bf volume} with likelihood above $L^*$.\\
Higher $L^* \Leftrightarrow$ lower volume.
\end{frame}
\begin{frame}{Numerical Integration}
If we had some points with likelihoods $L_i$, and we knew the corresponding
$X$-values, we could approximate the integral numerically, using the
trapezoidal rule or something similar.
%\begin{center}
%\includegraphics[scale=0.25,clip=true,angle=0]{Patricio/SampleLike2.pdf}
%\end{center}
\end{frame}
%######################################%##########################################################################################################################################
\begin{frame}{Nested Sampling Procedure}
This procedure gives us the likelihood values. \\

\begin{itemize}
	\item Sample $\theta=\{\theta_{1}, \ldots , \theta_{N}\}$ from the prior $\pi(\theta)$.
	\item Find the point $\theta_k$ with the worst
likelihood, and let $L^*$ be its likelihood.
	\item Replace $\theta_{k}$ with a new point from $\pi(\theta)$ but restricted to the region where $L(\theta)>L^*$.
\end{itemize}

Repeat the last two steps many times.
The \textit{discarded points} (the worst one at each iteration) are the output.
\end{frame}


\begin{frame}[t]{Generating the new point}
We need a new point from $\pi(\theta)$ but restricted to the region where $L(\theta)>L^*$. The point being replaced has the worst likelihood, so
{\bf all the other points satisfy the constraint!}
\vspace{20pt}

So we can use one of the other points to initialise an MCMC run, trying to
sample the prior, but rejecting any proposal with likelihood below $L^*$.
See code.
\end{frame}

\begin{frame}[t]{Generating the new point}
There are alternative versions of NS available, such as {\bf MultiNest}, that
use different methods (not MCMC) to generate the new point.\\
\vspace{20pt}

I also have a version of NS called {\bf Diffusive Nested Sampling}, which is
a better way of doing NS when using MCMC. I'm happy to discuss it offline.
\end{frame}


\begin{frame}[t]{Nested Sampling Procedure}
Nested Sampling gives us a sequence of points with increasing likelihoods,
but we need to somehow know their $X$-values!
\end{frame}

\begin{frame}[t]{Estimating the $X$ values}
Consider the simple one-dimensional problem with Uniform(0, 1) prior.\\

\vspace{20pt}
When we generate $N$ points from the prior, the distribution for the $X$-value
of the worst point is Beta$(N, 1)$. So we can use a draw from Beta$(N,1)$ as
a guess of the $X$ value.
\end{frame}

\begin{frame}[t]{Estimating the $X$ values}
Each iteration, the worst point should reduce the volume by a factor that has
a Beta$(N, 1)$ distribution. So we can do this:
\begin{eqnarray*}
X_1 &=& t_1\\
X_2 &=& t_2X_1\\
X_3 &=& t_3X_2\\
\end{eqnarray*}

and so on, where $t_i \sim $Beta$(N,1)$. Alternatively, we can use a simple
approximation.
\end{frame}

%##########################################################################################################################################
\begin{frame}[t]{Deterministic Approximation}
\begin{figure}[]    		
%		\includegraphics[scale=0.22,clip=true,angle=0]{Patricio/PriorExplore.pdf}
		\caption{Deterministic approximation. Each iteration reduces
the volume by a factor $\approx e^{-1/N}$. e.g. if $N=5$, the worst likelihood
accounts for about 1/5th of the remaining prior volume.}
\end{figure}
\end{frame}


\begin{frame}[t]{Posterior Distribution from Nested Sampling}
The posterior sample can be obtained by assigning weights $W_j$ to the
discarded points:
\begin{align*}
W_{j} = \frac{L_{j} w_{j}}{Z} 
\end{align*}
where $w_{j}=X_{j-1} - X_{j+1}$ is the ``prior weight/width'' associated with the
point. The ``effective sample size'' is given by
\begin{align*}
ESS = \exp \left( - \sum_{j=1}^{m} W_j \log W_j \right)
\end{align*}

\end{frame}

\begin{frame}[t]{Information}
NS can also calculate the {\bf information}, also known as the Kullback-Liebler
divergence from the prior to the posterior.

\begin{eqnarray*}
\mathcal{H} &=& \int p(\theta | D) \log\left[\frac{p(\theta | D)}{p(\theta)}\right]
\, d\theta\\
&\approx& \log\left(
\frac{\textnormal{volume of prior}}{\textnormal{volume of posterior}}
\right)
\end{eqnarray*}
\end{frame}

\begin{frame}[t]{Nested Sampling Code}
I have written a basic implementation of Nested Sampling in Python. Let's
use it on the transit problem and the asteroseismology problem.

\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
\vspace{-10pt}
%\begin{center}
%\includegraphics[scale=0.32]{ns.pdf}
%\end{center}

\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
A necessary but not sufficient condition for everything being okay is that you
see the entire peak in the posterior weights.\\

\vspace{20pt}

If it's not there, you haven't done enough NS iterations. i.e. your parameter
values have lower likelihoods than what is typical of the posterior distribution.
\end{frame}

\begin{frame}[t]{Nested Sampling Plots}
The shape of the log$(L)$ vs. log$(X)$ plot is also informative: if it is
straight for a long time, or concave up at some point, your problem contains
a phase transition, and it's a good thing you used Nested Sampling!
\end{frame}



\end{document}

